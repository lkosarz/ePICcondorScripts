# submitReco.job -- submit reco

Universe        		= vanilla
GetEnv          		= False
#InteractiveJob 			= False

# Use singularity image
+SingularityImage="/cvmfs/singularity.opensciencegrid.org/eicweb/jug_xl:nightly"
#+SingularityImage="/cvmfs/singularity.opensciencegrid.org/eicweb/jug_xl:23.11-stable"

# The requirement line specifies which machines we want to run this job on.  Any arbitrary classad expression can be used.
Requirements    = (CPU_Speed >= 1)

# Rank is an expression that states how to rank machines which have already met the requirements expression. Essentially, 
# rank expresses preference.  A higher numeric value equals better rank.  Condor will give the job the machine with the highest rank.
Rank		= CPU_Speed

# Jobs by default get 1.4Gb of RAM allocated, ask for more if needed but if a job needs
#more than 2Gb it will not be able to run on the older nodes
#request_memory = 1800M

# If you need multiple cores you can ask for them, but the scheduling may take longer the "larger" a job you ask for
#request_cpus = 1

# Used to give jobs a directory with respect to file input and output.
Initialdir      = /gpfs02/eic/lkosarzew/Calorimetry/testScripts

executable              = runRecoEICrecon.sh
arguments               = hcal_pion_433955_$(Process)_sim.edm4hep.root condorReco/output/hcal_pion_$(Cluster)_$(Process)_reco.edm4eic.root $(Process) $(Cluster)

transfer_input_files    = condorSim/output/hcal_pion_433955_$(Process)_sim.edm4hep.root,runRecoEICrecon.sh
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT_OR_EVICT
transfer_output_files   = condorReco


Error                   = condorReco/stderr/$(Cluster)_$(Process).err
Output                  = condorReco/stdout/$(Cluster)_$(Process).out
Log                     = condorReco/log/$(Cluster)_$(Process).log

#+Experiment     = "experiment"
#+Job_Type       = "cas"

Queue 10